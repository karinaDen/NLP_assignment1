{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nordvig's solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "king sport\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('data/big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "phrase = \"dking sport\"\n",
        "corrected_phrase = \" \".join(correction(word) for word in phrase.split())\n",
        "print(corrected_phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "doing sport\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load N-gram counts from file\n",
        "def load_ngrams(file_path, n=2):\n",
        "    ngrams = defaultdict(int)\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split(\"\\t\")\n",
        "            ngram = tuple(line[1:])\n",
        "            count = int(line[0])\n",
        "            ngrams[ngram] = count\n",
        "    return ngrams\n",
        "\n",
        "# Compute the probability of a given N-gram sequence\n",
        "def ngram_prob(ngram, ngrams, n=2):\n",
        "    count = ngrams[ngram]\n",
        "    prefix = ngram[:-1]\n",
        "    prefix_count = sum(ngrams[prefix_ngram] for prefix_ngram in ngrams if prefix_ngram[:-1] == prefix)\n",
        "    return count / prefix_count if prefix_count > 0 else 0\n",
        "\n",
        "# Generate possible correction candidates for a given misspelled word\n",
        "def generate_candidates(word, vocab):\n",
        "    candidates = []\n",
        "    for i in range(len(word) + 1):\n",
        "        for j in range(i + 1, len(word) + 2):\n",
        "            prefix = word[:i]\n",
        "            suffix = word[j:]\n",
        "            for candidate in vocab:\n",
        "                if candidate.startswith(prefix) and candidate.endswith(suffix):\n",
        "                    candidates.append(prefix + candidate + suffix)\n",
        "    return list(set(candidates))\n",
        "\n",
        "# Score each correction candidate based on its probability in the given context\n",
        "def score_candidates(sentence, candidates, ngrams, n=2):\n",
        "    scored_candidates = []\n",
        "    for candidate in candidates:\n",
        "        words = sentence.split()\n",
        "        for i, word in enumerate(words):\n",
        "            if word == '<unk>':\n",
        "                context = tuple(words[max(0, i - n + 1):i] + [candidate] + words[i + 1:i + n])\n",
        "                prob = ngram_prob(context, ngrams, n)\n",
        "                scored_candidates.append((candidate, prob))\n",
        "    scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored_candidates\n",
        "\n",
        "# Spelling correction function\n",
        "def correct_sentence(sentence, ngrams, n=2, vocab=None):\n",
        "    if vocab is None:\n",
        "        vocab = set(word for ngram in ngrams for word in ngram)\n",
        "    words = sentence.split()\n",
        "    corrected_words = []\n",
        "    for word in words:\n",
        "        if word not in vocab:\n",
        "            candidates = generate_candidates(word, vocab)\n",
        "            scored_candidates = score_candidates(' '.join(corrected_words + [word] + words[len(corrected_words) + 1:]), candidates, ngrams, n)\n",
        "            if scored_candidates:\n",
        "                corrected_word, _ = scored_candidates[0]\n",
        "            else:\n",
        "                corrected_word = word\n",
        "        else:\n",
        "            corrected_word = word\n",
        "        corrected_words.append(corrected_word)\n",
        "    return ' '.join(corrected_words)\n",
        "\n",
        "# Load N-gram counts\n",
        "ngrams = load_ngrams('data/bigrams.txt')\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"dking sport\"\n",
        "print(correct_sentence(sentence1, ngrams))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "## Justifications for the implementation choices in the given code:\n",
        "\n",
        "1. **N-gram dataset choice**: The code uses a specific N-gram dataset loaded from the file \"data/bigrams.txt\". The choice of the dataset depends on the available data and the desired context for spelling correction.  I've tried different another dataset given, and see no accurasy increased, so i lefn only one dataset with bigrams\n",
        "\n",
        "2. **N-gram Probability Calculation**: The code implements the `ngram_prob` function to compute the probability of a given N-gram sequence. It uses the count of the N-gram and the count of its prefix in the N-gram dataset to calculate the probability. The choice to use the count-based approach for probability calculation is a common practice in N-gram language modeling. By dividing the count of the N-gram by the count of its prefix, the code estimates the probability of the N-gram occurring in the given context. This approach assumes that the frequency of an N-gram in the dataset reflects its likelihood in the language.\n",
        "\n",
        "3. **Candidate Generation**: The code implements the `generate_candidates` function to generate possible correction candidates for a given misspelled word. It generates candidates by inserting, deleting, or replacing characters in the misspelled word and checking if the resulting word is in the vocabulary. The choice to generate candidates through character-level manipulation is a common approach in spelling correction. By considering different combinations of characters, the code aims to find potential corrections that are similar to the misspelled word.\n",
        "\n",
        "4. **Candidate Scoring**: The code implements the `score_candidates` function to score each correction candidate based on its probability in the given context using N-gram probabilities. It calculates the probability of each candidate by considering its context in the sentence and using the `ngram_prob` function. The choice to score candidates based on their context and N-gram probabilities is a common approach in spelling correction. By considering the surrounding words, the code aims to estimate the likelihood of each candidate being the correct correction for the misspelled word.\n",
        "\n",
        "5. **Spelling Correction**: The code implements the `correct_sentence` function to perform spelling correction on a given sentence. It iterates through each word in the sentence and checks if it is in the vocabulary. If not, it generates correction candidates and scores them using the `generate_candidates` and `score_candidates` functions. The choice to correct misspelled words based on candidate scoring is a common approach in spelling correction. By selecting the candidate with the highest probability, the code aims to choose the most likely correction for each misspelled word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\karin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "from nltk.corpus import gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence: This is a test sentence.\n",
            "Corrupted sentence: Thms is ap tost sentence.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import random\n",
        "import re\n",
        "\n",
        "def corrupt_sentence(sentence, prob=0.5, mistake_type_prob=0.9):\n",
        "    \"\"\"\n",
        "    Corrupts sentences by randomly introducing errors in words.\n",
        "    The probability of introducing an error is controlled by the `prob` parameter.\n",
        "    The probability of the error type (single edit or double edit) is controlled by the `mistake_type_prob` parameter.\n",
        "    \"\"\"    \n",
        "    words = re.findall(r'\\w+', sentence)\n",
        "    for i, word in enumerate(words):\n",
        "        if random.random() < prob and not word.isnumeric():\n",
        "            words[i] = random.choice(list(edits1(word) if random.random() < mistake_type_prob else edits2(word)))\n",
        "            if word.istitle():\n",
        "                words[i] = words[i].capitalize()\n",
        "    return re.sub(r'\\w+', lambda m: words.pop(0), sentence)\n",
        "\n",
        "sentence = \"This is a test sentence.\"\n",
        "corrupted_sentence = corrupt_sentence(sentence)\n",
        "print(f\"Original sentence: {sentence}\")\n",
        "print(f\"Corrupted sentence: {corrupted_sentence}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get test data\n",
        "test_set = gutenberg.sents('austen-emma.txt')[100:200]\n",
        "\n",
        "corrupted_test_set = []\n",
        "for sentence in test_set:\n",
        "    corrupted_sentence = corrupt_sentence(' '.join(sentence))\n",
        "    corrupted_test_set.append(corrupted_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_correction(corrected_sentence, correct_sentence):\n",
        "    \"\"\"\n",
        "    Evaluates the correction of a corrupted sentence\n",
        "    \"\"\"\n",
        "    correct_words = correct_sentence.split()\n",
        "    corrupted_words = corrected_sentence.split()\n",
        "    total_words = len(correct_words)\n",
        "    correct_words_count = 0\n",
        "    for i in range(total_words):\n",
        "        if correct_words[i].lower() == corrupted_words[i].lower():\n",
        "            correct_words_count += 1\n",
        "    return correct_words_count / total_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy for Nordvig: 0.6874456277359771\n",
            "Average accuracy for N-gram: 0.6620189110904304\n"
          ]
        }
      ],
      "source": [
        "total_accuracy_nordvig = 0\n",
        "total_accuracy_ngram = 0\n",
        "count = 0\n",
        "\n",
        "for correct_sentence, corrupted_sentence in zip(test_set, corrupted_test_set):\n",
        "    nordvig_corrected_sentence = \" \".join(correction(word) for word in corrupted_sentence.split())\n",
        "    ngram_corrected_sentence = correct_sentence(corrupted_sentence, ngrams)\n",
        "    accuracy_nordvig = evaluate_correction(nordvig_corrected_sentence, \" \".join(correct_sentence))\n",
        "    accuracy_ngram = evaluate_correction(ngram_corrected_sentence, \" \".join(correct_sentence))\n",
        "\n",
        "    total_accuracy_nordvig += accuracy_nordvig\n",
        "    total_accuracy_ngram += accuracy_ngram\n",
        "\n",
        "total_accuracy_ngram /= len(test_set)\n",
        "total_accuracy_nordvig /= len(test_set)    \n",
        "\n",
        "\n",
        "print(f\"Average accuracy for Nordvig: {total_accuracy_nordvig}\")\n",
        "print(f\"Average accuracy for N-gram: {total_accuracy_ngram}\")\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
